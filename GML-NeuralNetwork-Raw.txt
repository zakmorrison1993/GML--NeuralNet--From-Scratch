//NEURAL NET BUILD


i += 1; //cycle variable increase
j += 1; //cycle variable increase
c += 1; //cycle variable increase
q += 1; //cycle variable increase

var v = 0;
if v < 10
{
p+= 1;
}

if q <= f { // if cycleing variable less/equal than no of unit in first layer
fl[q] = 0; //sets first layer array to 0

if p > 8
{v+=1
p = 0;};

if instance_number(obj_gridpeice) < 100
{
grid = instance_create(p+5, v+5, obj_gridpeice);//create grid peice on position 
grid.pos = p; //set grid reference 
b[0,q] = random(1);// baises set 0
w[0,q] = random(1); // weights set 0

}
}

if start = true { //initiation variable set on

checked[c] = 0
answers[c] = 0

if c <= h { //if cycling variable less/equal than number of units in last layer
ffl[c] = 0;//sets laxst layer array to default 
b[2,c] = random(1);// baises set 2
w[2,c] = random(1); }// weights set 2}//if cycling variable over last layer number


if j <= l // if cycle variable = less than number of layers // if cycle variable = less than number of unit
{nn[0,i] = 0 // neural net hidden layers set defualt
nn[1,i] = 0}

m = (((l*s)+f)+h); //max cell add up (total)  

b[1,i] = random(1);// baises set 1
w[1,i] = random(1); // weights set 1


if i > (s-1) {donei = true;}
if c > (h - 1) {donec = true;}
if q > (f -1) {doneq = true;}

if donei = true && donec = true && doneq = true//if cycling variables less than total cell number
{start = false // initiaon vairable off
i = 0;} //reset cycle variable
}// inital config end


//reset variables

if calculate = false && start = false
{
q = 0;
}
if calculate = false && start = false && nextlayer1 = false
{
i = 0;
}

//NERUAL NET CALCULATE




if calculate = true {//if calculations intiatied 


if q > f-1
{qq += 1}
if i > s-1
{ii +=1}

if qq < f {
nn[0,i] = (1/(1 + exp(-(fl[qq]+w[0,q])+b[0,q])));// vector first layer 
if qq = f {
nextlayer1 = true
}}

if nextlayer1 = true && nextlayer2 = false
{
nn[1,i] = (1/(1 + exp(-(nn[0,ii]+w[1,i])+b[1,i]))); //vector hidden layers
if ii = s
{nextlayer2 = true
ii = 0;
}}

if nextlayer2 = true
{
beentrue = true
ffl[c] = (1/(1 + exp(-(nn[1,ii]+w[2,c])+b[2,c]))); //vector 2nd hidden to last layer
if ii = s
{nextlayer2 = false
ii = 0;
}}

}// end of calculation


//BACKPROPERGATION:LINARAL REGRESSION

if activated = true { //backpropergation activated 
calculate = false;
//cost function
total += ((fl[i] - ffl[c])*2) //add up total for each vector to make cost function
w = total*(0.5*m) //finish cost function


//gradiant decant//needs fixing??
for ( d = 0; d < step; d += 1){//for loop cycling variable is less than number of steps
gd =(w =(w - (lr*(2*w))))} //cost function = cost function minus ssteprate, cost fucntion squared)

//backpropergation




}//end of backpropergation

//OUTPUT NUMBERS
//change output neurons to numeric vales

if c > h-1
{g += 1;}





if calculate = true{

if ffl[g] > ffl[c]
{checked[g] = true;
if checked[c] = true {answer = answers[c];}
}

z+=1;



answer[c] = c;


if z > 98 && nextlayer2 = false && beentrue = true
{calculate = false;}
}


//LOOPS
if start = false { //if neural net build = false


}

if i = s { //if cycle variable greater than total number of cells in hidden layer
i = 0;} //reset 
if j = l { //if cycle variable greater than total number of layers
j = 0} //reset
if q = f { //if cycle variable greater than total number of cells in first
q = 0}; //reset
if c = h { //if cycle variable greater than total number of cells in last layer
c = 0;}//reset
if g = h {//if cycle through total amount of output neurons
g = 0;} //reset



